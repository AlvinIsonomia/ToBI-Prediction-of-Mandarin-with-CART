{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data......\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xbf in position 7: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e9a65dd47cd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corpus.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m  \u001b[0mf1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#打开文件\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mf11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#将打开文件的内容读到内存中，with 在执行完命令后，会关闭文件\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corResult.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#打开一个文件，用于写入，后面的'wb'表示每次写入前格式化文本，如果此文件不存在，则创建一个此文件名的文件\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py3/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xbf in position 7: invalid start byte"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "print(\"processing data......\")\n",
    "\n",
    "with open('/home/data/liuchang/data/corpus.txt') as  f1:#打开文件\n",
    "    f11 = f1.readlines()#将打开文件的内容读到内存中，with 在执行完命令后，会关闭文件\n",
    "    \n",
    "f2 = open('/home/data/liuchang/data/corResult.txt','wb')#打开一个文件，用于写入，后面的'wb'表示每次写入前格式化文本，如果此文件不存在，则创建一个此文件名的文件\n",
    "\n",
    "for x in f11:\n",
    "    #x = x.encode(\"utf-8\")\n",
    "    #x = x.decode(\"utf-8\")\n",
    "    filtrate = re.compile(u'[^\\u4E00-\\u9FA5]')#非中文\n",
    "    filtered_str = filtrate.sub(r'', x)#replace\n",
    "    filtered_str = filtered_str.encode(\"utf-8\")\n",
    "    f2.write(filtered_str)\n",
    "    f2.write('\\n'.encode(\"utf-8\"))\n",
    "\n",
    "f2.close()    \n",
    "print(\"end!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最早长成这样:\n",
      "000001\t卡尔普#2陪外孙#1玩滑梯#4。\n",
      "\tka2 er2 pu3 pei2 wai4 sun1 wan2 hua2 ti1\n"
     ]
    }
   ],
   "source": [
    "#正则表达式第一次处理\n",
    "import re\n",
    "before = '000001\t卡尔普#2陪外孙#1玩滑梯#4。\\n\tka2 er2 pu3 pei2 wai4 sun1 wan2 hua2 ti1'\n",
    "print (\"最早长成这样:\\n\"+before)\n",
    "#先把不用的拼音,行号去了\n",
    "with open('/home/liuchang/data/utf8corpus.txt') as reader, open('/home/liuchang/data/newcorpus.txt','w') as writer:\n",
    "    for index, line in enumerate(reader):\n",
    "        if index % 2 == 0:#只取偶数行的数据\n",
    "            line = re.sub(r\"(?m)^\\D*\\d+\\s*[.|:|\\]|)]?\\s?\", \"\", line)#去除文本前乱七八糟的东西\n",
    "            writer.write(line)\n",
    "print (\"现在长这样：\\n 卡尔普#2陪外孙#1玩滑梯#4。\")\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data......\n",
      "end!\n",
      "files opened!\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "import jieba.posseg as pseg #POS标注接口\n",
    "import codecs,sys\n",
    "import re\n",
    "\n",
    "print(\"processing data......\")\n",
    "\n",
    "with open('/home/data/liuchang/data/newcorpus.txt') as  f1:#打开文件\n",
    "    f11 = f1.readlines()#将打开文件的内容读到内存中，with 在执行完命令后，会关闭文件\n",
    "    \n",
    "f2 = open('/home/data/liuchang/data/mycorpus.txt','wb')#打开一个文件，用于写入，后面的'wb'表示每次写入前格式化文本，如果此文件不存在，则创建一个此文件名的文件\n",
    "\n",
    "for x in f11:\n",
    "    #x = x.encode(\"utf-8\")\n",
    "    #x = x.decode(\"utf-8\")\n",
    "    #filtrate = re.compile(u'[^\\u4E00-\\u9FA5]')#非中文\n",
    "    filtered_str = re.sub(r'#[0-9]',\"\",x)#replace\n",
    "    filtered_str = filtered_str.encode(\"utf-8\")\n",
    "    f2.write(filtered_str)\n",
    "    #f2.write('\\n'.encode(\"utf-8\"))\n",
    "f2.close()    \n",
    "print(\"end!\")\n",
    "\n",
    "\n",
    "with open('/home/data/liuchang/data/mycorpus.txt') as  f1:\n",
    "    f1l = f1.readlines()\n",
    "f2 = open('/home/data/liuchang/data/mycorpusSeg.txt','wb')\n",
    "print(\"files opened!\")\n",
    "for x in f1l:\n",
    "    seg_list = jieba.cut(x,cut_all=False)\n",
    "    line_seg = ' '.join(seg_list)\n",
    "    f2.write(line_seg.encode(\"utf-8\"))\n",
    "    #f2.write('\\n'.encode(\"utf-8\"))\n",
    "\n",
    "f2.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "卡尔普#2陪外孙#1玩滑梯#4。\n",
      "['卡尔普', '陪外孙', '玩滑梯', '。']\n",
      "['2', '1', '4']\n",
      "0 1 2 3 4 56 7 8 9\n",
      "['1ing', 'yi', '0', 'ersan', '0', 'swl', 'qi', '0', 'bajiu']\n"
     ]
    }
   ],
   "source": [
    "#对齐两个分词文本\n",
    "#Step1：元数据分词，标签另存\n",
    "newcorpusline=\"卡尔普#2陪外孙#1玩滑梯#4。\"\n",
    "print (newcorpusline)\n",
    "print (re.split(r'#\\d', newcorpusline))#只留字符\n",
    "\n",
    "classpattern = re.compile(r'\\d')\n",
    "print (classpattern.findall(newcorpusline))#只留标签（去除#）\n",
    "\n",
    "#把两个空格不对齐的地方加空格，难点其实在于补充源文件没有的标签\n",
    "str0='0 1 23 456 7 89'\n",
    "tag=['1ing','yi','ersan','swl','qi','bajiu']\n",
    "#若处理正确 最终str=['0 1 2 3 4 56 7 8 9'] tag=['1ing','yi','0','ersan','0','swl','qi','0','bajiu']\n",
    "str1='01 2 3 4 567 8 9'\n",
    "\n",
    "index1=0\n",
    "indexT=0\n",
    "for index1 in range(len(str0) * 2):\n",
    "    try:\n",
    "        if str0[index1] == ' ':\n",
    "            indexT = indexT + 1\n",
    "        if str0[index1] == str1[index1]:\n",
    "            index1 = index1 + 1\n",
    "        elif str0[index1] == ' ':        \n",
    "            str1_list=list(str1)\n",
    "            str1_list.insert(index1,' ')\n",
    "            str1=\"\".join(str1_list)\n",
    "            index1 = index1 + 1            \n",
    "        else:\n",
    "            str0_list=list(str0)\n",
    "            str0_list.insert(index1,' ')\n",
    "            str0=\"\".join(str0_list)            \n",
    "            tag.insert(indexT,'0')\n",
    "            indexT = indexT + 1\n",
    "            index1 = index1 + 1\n",
    "    except:\n",
    "        break\n",
    "print(str0)\n",
    "print(tag)\n",
    "#搞定！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "Could not find stanford-postagger.jar jar file at /home/liuchang/MyPython/stanford-postagger-full/stanford-postagger.jar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5f715b516851>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#stanford\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStanfordPOSTagger\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mPOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mcntagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPOS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mr'/home/data/liuchang/data/stanford-postagger-full/models/chinese-distsim.tagger'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath_to_jar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mr'/home/liuchang/MyPython/stanford-postagger-full/stanford-postagger.jar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mseg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcntagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"老虎 幼崽 与 宠物犬 玩耍 。\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/data/liuchang/anaconda3/envs/py3/lib/python3.7/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStanfordPOSTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/data/liuchang/anaconda3/envs/py3/lib/python3.7/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_filename, path_to_jar, encoding, verbose, java_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m             )\n\u001b[1;32m     73\u001b[0m         self._stanford_jar = find_jar(\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_JAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_stanford_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         )\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/data/liuchang/anaconda3/envs/py3/lib/python3.7/site-packages/nltk/internals.py\u001b[0m in \u001b[0;36mfind_jar\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    851\u001b[0m     return next(\n\u001b[1;32m    852\u001b[0m         find_jar_iter(\n\u001b[0;32m--> 853\u001b[0;31m             \u001b[0mname_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_regex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m         )\n\u001b[1;32m    855\u001b[0m     )\n",
      "\u001b[0;32m/home/data/liuchang/anaconda3/envs/py3/lib/python3.7/site-packages/nltk/internals.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             raise LookupError(\n\u001b[0;32m--> 739\u001b[0;31m                 \u001b[0;34m'Could not find %s jar file at %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m             )\n\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: Could not find stanford-postagger.jar jar file at /home/liuchang/MyPython/stanford-postagger-full/stanford-postagger.jar"
     ]
    }
   ],
   "source": [
    "# #词性与语法分析，下一步要精简系统去除冗余\n",
    "# '''\n",
    "# import jieba \n",
    "# import jieba.analyse \n",
    "# import jieba.posseg as pseg #POS标注接口 \n",
    "\n",
    "# import codecs,sys \n",
    "# import re\n",
    "\n",
    "# #词性标注至少在demo阶段不能用JIEBA= =原因见下 \n",
    "# seg = pseg.cut(\"老虎 幼崽 与 宠物犬 玩耍 。\" ) \n",
    "# for ele in seg: \n",
    "#     print (ele)\n",
    "# '''\n",
    "#stanford \n",
    "# from nltk.tag import StanfordPOSTagger as POS \n",
    "# cntagger = POS(model_filename=r'/home/data/liuchang/data/stanford-postagger-full/models/chinese-distsim.tagger',path_to_jar=r'/home/liuchang/MyPython/stanford-postagger-full/stanford-postagger.jar') \n",
    "# seg1 = cntagger.tag(\"老虎 幼崽 与 宠物犬 玩耍 。\".split()) \n",
    "# print(seg1) \n",
    "# seg1[1][1]#第一个数字是第几个词，第二个数字别动\n",
    "# tags = [ ]\n",
    "# i = 0\n",
    "# for words in seg1:\n",
    "#     str1 = words[1]#元组类型不可更改，必须先赋值\n",
    "#     tags.insert(i,re.sub(r'^.*#','',str1))\n",
    "#     print(tags[i])\n",
    "#     i=i+1\n",
    "# #其实是有bug的，直觉上中文应该在第一个元素，但这里都是空的，词性独立开来，不过这也只是一个正则表达式的问题\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#语法分析+深度\n",
    "from nltk.parse.stanford import StanfordParser as PAR\n",
    "import os #POS需要java环境\n",
    "from nltk.tree import *\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "java_path = \"/home/data/liuchang/jdk1.8.0_191/bin/java\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "full_path=u\"/home/data/liuchang/data/stanford-parser-full\"\n",
    "cnparser = PAR(\n",
    "path_to_jar=full_path + u\"/stanford-parser.jar\",\n",
    "path_to_models_jar=full_path + u\"/stanford-parser-3.9.2-models.jar\",\n",
    "model_path=full_path + u'/chinesePCFG.ser.gz')\n",
    "cnd_parser = StanfordDependencyParser(path_to_jar = full_path + u\"/stanford-parser.jar\",\n",
    "                                      path_to_models_jar = full_path + u\"/stanford-parser-3.9.2-models.jar\",\n",
    "                                      model_path = full_path + u\"/chinesePCFG.ser.gz\")\n",
    "\n",
    "sent = u' 每一个人，作为社会的一个成员，有权享受其人格和尊严的自由发展所必需的权利 。'\n",
    "words = sent.split()\n",
    "cntree = list(cnparser.parse(words))\n",
    "cntree = cntree[0]\n",
    "ptree = ParentedTree.fromstring(str(cntree[0]))\n",
    "ptree.pprint() #打印树的结构\n",
    "leaf_values = ptree.leaves()\n",
    "# len(leaf_values)\n",
    "# print(leaf_values)#句子中每个词是叶子节点的数值\n",
    "leaf_index = leaf_values.index('成员')\n",
    "tree_location = ptree.leaf_treeposition(leaf_index)#树的索引，可以递归（嵌套？）找到对应叶子节点，但是只能返回的一个词\n",
    "# print(tree_location)\n",
    "# print(ptree[2][1][2][0].parent().label())\n",
    "# print(len(tree_location))\n",
    "# print('树的类型',type(ptree))\n",
    "# print(ptree[0].label())#用这个可以得到他们的语法结构，从POS到Syntactic tags都有\n",
    "parses = cnd_parser.parse(words)\n",
    "dependencies = [[(governor, dep, dependent) for governor, dep, dependent in parse.triples()] for parse in parses]\n",
    "len(dependencies[0])\n",
    "index = 1\n",
    "# for dependency in dependencies[0]:\n",
    "#     print('第',index,'个依存关系:')\n",
    "#     print('Governor:',dependency[0],',dep:',dependency[1],',dependent',dependency[2])\n",
    "#     index += 1\n",
    "print(words)\n",
    "global father\n",
    "father = [ ]\n",
    "global parent_num\n",
    "parent_num = 1\n",
    "def traverse(t):\n",
    "    global onefather\n",
    "    global parent_num\n",
    "    try:\n",
    "        t.label()        \n",
    "    except AttributeError:\n",
    "        return\n",
    "    else:\n",
    "        if t.height() == 2:   #child nodes\n",
    "#             print(t.parent().label())\n",
    "            father.append(t.parent().label())\n",
    "#             print(t.parent().height())\n",
    "            return\n",
    "        for child in t:\n",
    "            traverse(child)\n",
    "\n",
    "traverse(ptree)\n",
    "print(father)\n",
    "onefather = [1 ]\n",
    "for i in range(1,len(father)):\n",
    "    if(father[i-1] == father[i]):\n",
    "        onefather.append(1)\n",
    "    else:\n",
    "        onefather.append(0)\n",
    "print(onefather)\n",
    "import re\n",
    "zh_pattern = re.compile(u'[\\u4e00-\\u9fa5]+')\n",
    "def contain_zh(word):\n",
    "    '''\n",
    "    判断传入字符串是否包含中文\n",
    "    :param word: 待判断字符串\n",
    "    :return: True:包含中文  False:不包含中文\n",
    "    '''\n",
    "    #word = word.encode(\"utf-8\")\n",
    "    global zh_pattern\n",
    "    match = zh_pattern.search(word)\n",
    "\n",
    "    return match\n",
    "\n",
    "for i in range(0,len(words)-1):\n",
    "    if not(contain_zh(words[i])):\n",
    "        del onefather[i]\n",
    "print(onefather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #获取句法生成树的深度特征\n",
    "# #辣鸡corenlp\n",
    "# '''\n",
    "# java -Xmx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer \\\n",
    "# -serverProperties StanfordCoreNLP-chinese.properties \\\n",
    "# -preload tokenize,ssplit,pos,lemma,ner,parse \\\n",
    "# -status_port 9001  -port 9999 -timeout 15000\n",
    "# '''\n",
    "# #话说上面这个我感觉status_port可能也得改，但暂时没什么问题（\n",
    "# from nltk.parse.corenlp import CoreNLPParser\n",
    "# from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "# from nltk.tree import *\n",
    "# parser = CoreNLPParser(url = 'http://localhost:9999')\n",
    "# dep_parser = CoreNLPDependencyParser(url='http://localhost:9999')\n",
    "# sent = u' 每一个 人 ， 作为 社会 的 一个 成员 ， 有权 享受 其 人格 和 尊严 的 自由 发展 所 必需 的 权利 。'\n",
    "# words = list(sent.split())\n",
    "# print(words)\n",
    "# cntree = parser.parse(sent.split())\n",
    "# cntree = list(cntree)\n",
    "# ptree = ParentedTree.fromstring(str(cntree[0]))\n",
    "# ptree.pprint() #打印树的结构\n",
    "# leaf_values = ptree.leaves()\n",
    "# # len(leaf_values)\n",
    "# # print(leaf_values)#句子中每个词是叶子节点的数值\n",
    "# leaf_index = leaf_values.index('个')\n",
    "# tree_location = ptree.leaf_treeposition(leaf_index)#树的索引，可以递归（嵌套？）找到对应叶子节点，但是只能返回的一个词\n",
    "# print(tree_location)\n",
    "# print(len(tree_location))\n",
    "# print('树的类型',type(ptree))\n",
    "# print(ptree[0].label())#用这个可以得到他们的语法结构，从POS到Syntactic tags都有\n",
    "# parses = dep_parser.parse(sent.split())\n",
    "# dependencies = [[(governor, dep, dependent) for governor, dep, dependent in parse.triples()] for parse in parses]\n",
    "# len(dependencies[0])\n",
    "# index = 1\n",
    "# # for dependency in dependencies[0]:\n",
    "# #     print('第',index,'个依存关系:')\n",
    "# #     print('Governor:',dependency[0],',dep:',dependency[1],',dependent',dependency[2])\n",
    "# #     index += 1\n",
    "# print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data......\n",
      "processing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data/liuchang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:122: DeprecationWarning: The StanfordParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.StanforCoreNLPParser\u001b[0m instead.\n",
      "/home/data/liuchang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:125: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.StanforCoreNLPDependencyParser\u001b[0m instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b901a89de9984335836d910dedea2c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "[1, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "#STEP1:获得最终分词和标签\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tag import StanfordPOSTagger as POS #词性标注\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from nltk.tag import StanfordPOSTagger as STFDPOS \n",
    "from nltk.tree import *\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "\n",
    "import os #POS需要java环境\n",
    "java_path = \"/home/data/liuchang/jdk1.8.0_191/bin/java\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "'''\n",
    "full_path=u\"/home/data/liuchang/data/stanford-parser-full\"\n",
    "cnparser = PAR(\n",
    "path_to_jar=full_path + u\"/stanford-parser.jar\",\n",
    "path_to_models_jar=full_path + u\"/stanford-parser-3.9.2-models.jar\",\n",
    "model_path=full_path + u'/chinesePCFG.ser.gz')\n",
    "cnd_parser = StanfordDependencyParser(path_to_jar = full_path + u\"/stanford-parser.jar\",\n",
    "                                      path_to_models_jar = full_path + u\"/stanford-parser-3.9.2-models.jar\",\n",
    "                                      model_path = full_path + u\"/chinesePCFG.ser.gz\")\n",
    "'''\n",
    "\n",
    "from zhon.hanzi import punctuation as cnpun\n",
    "\n",
    "def is_pun(char):\n",
    "    return (char in cnpun)\n",
    "        \n",
    "zh_pattern = re.compile(u'[\\u4e00-\\u9fa5]+')\n",
    "def contain_zh(word):\n",
    "    '''\n",
    "    判断传入字符串是否包含中文\n",
    "    :param word: 待判断字符串\n",
    "    :return: True:包含中文  False:不包含中文\n",
    "    '''\n",
    "    #word = word.encode(\"utf-8\")\n",
    "    global zh_pattern\n",
    "    match = zh_pattern.search(word)\n",
    "\n",
    "    return match\n",
    "\n",
    "\n",
    "\n",
    "print(\"processing data......\")\n",
    "\n",
    "with open('/home/data/liuchang/data/newcorpus.txt') as  f1:\n",
    "    f11 = f1.readlines()\n",
    "with open('/home/data/liuchang/data/mycorpusSeg.txt') as  f2:\n",
    "    f21 = f2.readlines()\n",
    "    \n",
    "#f3 = open('/home/data/liuchang/data/final.txt','wb')\n",
    "#f4 = open('/home/data/liuchang/data/tag.txt','wb')\n",
    "textlist = [] #在一起的分词结果文本，有标点\n",
    "classlist = [] #每个中文词的停顿标注\n",
    "textwordslist = [] #文本的分词列表，有标点\n",
    "wordlenlist = []#当前词的长度 \n",
    "POSlist = [] #当前词的词性\n",
    "cnwordlist = []#中文词标点\n",
    "for line in range(0,10000):\n",
    "    #x = x.encode(\"utf-8\")\n",
    "    #x = x.decode(\"utf-8\")\n",
    "    str0 = re.sub(r'#\\d',' ', f11[line])#这个可以当作分词\n",
    "    str0 = str0.strip('\\n')\n",
    "    \n",
    "    str1 = f21[line]\n",
    "    str1 = str1.strip('\\n')\n",
    "    classpattern = re.compile(r'\\d')\n",
    "    tag = classpattern.findall(f11[line])\n",
    "    index1=0\n",
    "    indexT=0\n",
    "    for index1 in range(len(str0) * 2):\n",
    "        try:\n",
    "            if str0[index1] == ' ':\n",
    "                if not(is_pun(str0[index1])):\n",
    "                    indexT = indexT + 1\n",
    "            if str0[index1] == str1[index1]:\n",
    "                index1 = index1 + 1\n",
    "            elif str0[index1] == ' ':        \n",
    "                str1_list=list(str1)\n",
    "                str1_list.insert(index1,' ')\n",
    "                str1=\"\".join(str1_list)\n",
    "                index1 = index1 + 1\n",
    "            else:\n",
    "                str0_list=list(str0)\n",
    "                str0_list.insert(index1,' ')\n",
    "                str0=\"\".join(str0_list)\n",
    "                if not(is_pun(str0[index1-1])):\n",
    "                    tag.insert(indexT,'1')\n",
    "                    indexT = indexT + 1\n",
    "                index1 = index1 + 1\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    textlist.append(str0)#有空格分词的文本\n",
    "    classlist.append(tag)#只留标签（去除#）\n",
    "\n",
    "for text in textlist:\n",
    "    textword=[]\n",
    "    textword=text.split()\n",
    "    tempcnword = []    \n",
    "    for word in textword:\n",
    "        if contain_zh(word):\n",
    "            tempcnword.append(word)\n",
    "    textwordslist.append(textword)\n",
    "    cnwordlist.append(tempcnword)\n",
    "print(\"processing complete!\")\n",
    "\n",
    "\n",
    "from nltk.parse.stanford import StanfordParser as PAR\n",
    "import os #POS需要java环境\n",
    "from nltk.tree import *\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "java_path = \"/home/data/liuchang/jdk1.8.0_191/bin/java\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "full_path=u\"/home/data/liuchang/data/stanford-parser-full\"\n",
    "cnparser = PAR(\n",
    "path_to_jar=full_path + u\"/stanford-parser.jar\",\n",
    "path_to_models_jar=full_path + u\"/stanford-parser-3.9.2-models.jar\",\n",
    "model_path=full_path + u'/chinesePCFG.ser.gz')\n",
    "cnd_parser = StanfordDependencyParser(path_to_jar = full_path + u\"/stanford-parser.jar\",\n",
    "                                      path_to_models_jar = full_path + u\"/stanford-parser-3.9.2-models.jar\",\n",
    "                                      model_path = full_path + u\"/chinesePCFG.ser.gz\")\n",
    "\n",
    "\n",
    "def traverse(t):\n",
    "    global onefather\n",
    "    global parent_num\n",
    "    try:\n",
    "        t.label()        \n",
    "    except AttributeError:\n",
    "        return\n",
    "    else:\n",
    "        if t.height() == 2:   #child nodes\n",
    "#             print(t.parent().label())\n",
    "            father.append(t.parent().label())\n",
    "#             print(t.parent().height())\n",
    "            return\n",
    "        for child in t:\n",
    "            traverse(child)\n",
    "import re\n",
    "zh_pattern = re.compile(u'[\\u4e00-\\u9fa5]+')\n",
    "def contain_zh(word):\n",
    "    '''\n",
    "    判断传入字符串是否包含中文\n",
    "    :param word: 待判断字符串\n",
    "    :return: True:包含中文  False:不包含中文\n",
    "    '''\n",
    "    #word = word.encode(\"utf-8\")\n",
    "    global zh_pattern\n",
    "    match = zh_pattern.search(word)\n",
    "\n",
    "    return match\n",
    "fatherlist = []\n",
    "\n",
    "\n",
    "# for words in tqdm(textwordslist):\n",
    "for i in tqdm(range(0,10000)):\n",
    "    words = cnwordlist[i]\n",
    "#     print(words)\n",
    "    cntree = list(cnparser.parse(words))\n",
    "    cntree = cntree[0]\n",
    "    ptree = ParentedTree.fromstring(str(cntree[0]))\n",
    "    global father\n",
    "    father = [ ]\n",
    "    global parent_num\n",
    "    parent_num = 1\n",
    "    traverse(ptree)\n",
    "    #print(father)\n",
    "    onefather = [1 ]\n",
    "    for i in range(1,len(father)):\n",
    "        if(father[i-1] == father[i]):\n",
    "            onefather.append(1)\n",
    "        else:\n",
    "            onefather.append(0)\n",
    "    for j in range(0,len(words)-1):\n",
    "        if(contain_zh(words[j]) == None):\n",
    "#             print(j)\n",
    "            del onefather[j]\n",
    "            print('del!')\n",
    "#     print(onefather)\n",
    "    fatherlist.append(onefather)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data......\n",
      "processing complete!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from nltk.tag import StanfordPOSTagger as POS #词性标注\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from nltk.tag import StanfordPOSTagger as STFDPOS \n",
    "\n",
    "import os #POS需要java环境\n",
    "java_path = \"/home/data/liuchang/jdk1.8.0_191/bin/java\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "from zhon.hanzi import punctuation as cnpun\n",
    "\n",
    "def is_pun(char):\n",
    "    return (char in cnpun)\n",
    "        \n",
    "zh_pattern = re.compile(u'[\\u4e00-\\u9fa5]+')\n",
    "def contain_zh(word):\n",
    "    '''\n",
    "    判断传入字符串是否包含中文\n",
    "    :param word: 待判断字符串\n",
    "    :return: True:包含中文  False:不包含中文\n",
    "    '''\n",
    "    #word = word.encode(\"utf-8\")\n",
    "    global zh_pattern\n",
    "    match = zh_pattern.search(word)\n",
    "\n",
    "    return match\n",
    "\n",
    "\n",
    "\n",
    "print(\"processing data......\")\n",
    "\n",
    "with open('/home/data/liuchang/data/newcorpus.txt') as  f1:\n",
    "    f11 = f1.readlines()\n",
    "with open('/home/data/liuchang/data/mycorpusSeg.txt') as  f2:\n",
    "    f21 = f2.readlines()\n",
    "    \n",
    "#f3 = open('/home/data/liuchang/data/final.txt','wb')\n",
    "#f4 = open('/home/data/liuchang/data/tag.txt','wb')\n",
    "textlist = [] #在一起的分词结果文本，有标点\n",
    "classlist = [] #每个中文词的停顿标注\n",
    "textwordslist = [] #文本的分词列表，有标点\n",
    "wordlenlist = []#当前词的长度 \n",
    "POSlist = [] #当前词的词性\n",
    "cnwordlist = []#中文词标点\n",
    "for line in range(0,10000):\n",
    "    #x = x.encode(\"utf-8\")\n",
    "    #x = x.decode(\"utf-8\")\n",
    "    str0 = re.sub(r'#\\d',' ', f11[line])#这个可以当作分词\n",
    "    str0 = str0.strip('\\n')\n",
    "    \n",
    "    str1 = f21[line]\n",
    "    str1 = str1.strip('\\n')\n",
    "    classpattern = re.compile(r'\\d')\n",
    "    tag = classpattern.findall(f11[line])\n",
    "    index1=0\n",
    "    indexT=0\n",
    "    for index1 in range(len(str0) * 2):\n",
    "        try:\n",
    "            if str0[index1] == ' ':\n",
    "                if not(is_pun(str0[index1])):\n",
    "                    indexT = indexT + 1\n",
    "            if str0[index1] == str1[index1]:\n",
    "                index1 = index1 + 1\n",
    "            elif str0[index1] == ' ':        \n",
    "                str1_list=list(str1)\n",
    "                str1_list.insert(index1,' ')\n",
    "                str1=\"\".join(str1_list)\n",
    "                index1 = index1 + 1\n",
    "            else:\n",
    "                str0_list=list(str0)\n",
    "                str0_list.insert(index1,' ')\n",
    "                str0=\"\".join(str0_list)\n",
    "                if not(is_pun(str0[index1-1])):\n",
    "                    tag.insert(indexT,'1')\n",
    "                    indexT = indexT + 1\n",
    "                index1 = index1 + 1\n",
    "        except:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    textlist.append(str0)#有空格分词的文本\n",
    "    classlist.append(tag)#只留标签（去除#）\n",
    "\n",
    "for text in textlist:\n",
    "    textword=[]\n",
    "    textword=text.split()\n",
    "    tempcnword = []    \n",
    "    for word in textword:\n",
    "        if contain_zh(word):\n",
    "            tempcnword.append(word)\n",
    "    textwordslist.append(textword)\n",
    "    cnwordlist.append(tempcnword)\n",
    "print(\"processing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-29481f27a179>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0merrornum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnwordlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0merrorline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasslist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0merrorline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'#\\d'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf11\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0merrorline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "errornum = 0\n",
    "errorline = []\n",
    "for i in range(0,10000):\n",
    "    if (len(cnwordlist[i]) != len(fatherlist[i])):\n",
    "        errornum += 1\n",
    "        errorline.append(i)\n",
    "print (errornum)\n",
    "for i in range(0,1):\n",
    "    print(cnwordlist[errorline[i]])\n",
    "    print(classlist[errorline[i]])\n",
    "    print(re.sub(r'#\\d',' ', f11[errorline[i]]))\n",
    "    print(f21[errorline[i]])\n",
    "    print('啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊fuck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run a.py # 跑别的py脚本，并且工作区存在python中（好强啊x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pickle.dump(fatherlist,open('/home/data/liuchang/data/fatherlist.txt', 'wb') ) \n",
    "\n",
    "fatherlist = pickle.load(open('/home/data/liuchang/data/fatherlist.txt', 'rb'))\n",
    "print(len(fatherlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
